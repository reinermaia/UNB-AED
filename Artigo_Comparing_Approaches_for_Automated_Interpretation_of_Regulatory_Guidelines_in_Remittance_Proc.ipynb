{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bQhmfvws17d"
   },
   "source": [
    "# **ETAPA 1 - Preparação para as rotinas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofRhCLLQXp_t"
   },
   "source": [
    "# 1.1 Chave de utilização - GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obSNeLJmD1Cm"
   },
   "outputs": [],
   "source": [
    "API_KEY = \"zzz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqhIoNUhXvi4"
   },
   "source": [
    "# 1.2 Instalando pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mIXiVwvOWMD5",
    "outputId": "76bf2bde-1d01-4dac-fcb5-a3c14b7de193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pWUkQHHUXofI",
    "outputId": "a4b8989d-dcae-444d-a08d-545fbe6e5313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "4rg4F1x7-FDT",
    "outputId": "bc0d1636-f0e9-4eaa-f6d0-b8b862355848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IIHsthAk6LIB",
    "outputId": "c3901896-b0c4-4be2-d989-e4c865528ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzP-iP_FX_WC"
   },
   "source": [
    "# 1.3 Código para visualizar versões do GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ILRnRbBENKjE",
    "outputId": "df507649-5c41-46de-cc49-724fdbeb7c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-2024-07-18\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172717,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172741,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727659998,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698798177,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1671217299,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727131766,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-1106-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698957206,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-large\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705953180,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"babbage-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-11-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1731975040,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725649008,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"davinci-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini-2024-09-12\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725648979,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-0125-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037612,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677532384,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698785189,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-08-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1722814719,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725648897,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1683758102,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-preview-2024-09-12\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725648865,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715367049,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1731689265,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-2024-09-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1732734466,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053533,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1687882411,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-0613\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1686588896,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677610602,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706048358,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-small\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705948997,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712361441,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699046015,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712601677,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698959748,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692901427,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727460443,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727389042,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1681940951,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053241,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1694122472,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"chatgpt-4o-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1723515131,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-05-13\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715368132,\n",
      "      \"owned_by\": \"system\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"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\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "link = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "requisicao = requests.get(link, headers=headers)\n",
    "\n",
    "\n",
    "print(requisicao)\n",
    "\n",
    "print(requisicao.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLmvSIZTCBhw",
    "outputId": "c0bb0e72-3860-42cc-fbf0-d6fe797a9691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.14.6 in /usr/local/lib/python3.11/dist-packages (2.14.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.14.6) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (0.27.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.6) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.6) (1.18.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.6) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.6) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.6) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.6) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.6) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.6) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.6) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.6) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.6) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.14.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPExUkmvYAxT"
   },
   "source": [
    "# 1.4 Código para testar a API do GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "mJIvgWHoS2Z9",
    "outputId": "72a83faa-386a-43fb-c2f1-fd047b34b206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "  \"id\": \"chatcmpl-AYJCYuHxauztrjosnlERmKXu1PnLJ\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1732740326,\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"O processo de envio de remessas para o Banco Central do Brasil (BC) refere-se, geralmente, à transferência de valores, informações ou dados que precisam ser reportados à autoridade reguladora financeira do país. O BC desempenha um papel crucial na supervisão do sistema financeiro, e o envio de remessas pode incluir uma variedade de transações e reportes, como:\\n\\n1. **Operações de Câmbio**: Instituições financeiras e outras entidades que realizam operações de câmbio precisam reportar ao BC informações sobre essas transações, como valores, tipos de moeda e motivos da operação.\\n\\n2. **Remessas de Valores ao Exterior**: Para enviar dinheiro para fora do país, é necessário seguir certos procedimentos e reportar as transações ao BC, observando as regulamentações sobre a saída de divisas. Isso pode incluir remessas pessoais, pagamentos de serviços, compra de bens, entre outros.\\n\\n3. **Captação de Recursos no Exterior**: Empresas que buscam captar recursos fora do Brasil devem reportar ao BC, informando detalhes sobre a operação, como valores, prazos e uso dos recursos.\\n\\n4. **Transações de Instituições Financeiras**: As instituições financeiras são obrigadas a enviar dados sobre suas operações para o BC, visando a supervisão e a regulação do sistema financeiro. Isso inclui relatórios periódicos sobre ativos, passivos e outras operações.\\n\\n5. **Sistema de Informações**: O BC possui sistemas específicos para a coleta de dados, como o Sistema de Registro de Operações de Câmbio (ROF). Esses sistemas facilitam o registro e o envio de informações relevantes.\\n\\n### Estrutura do Processo\\n\\nO envio de remessas e informações para o BC geralmente envolve os seguintes passos:\\n\\n1. **Preparação**: Coletar e organizar os dados necessários, seguindo as normas e regulamentações estabelecidas pelo BC.\\n\\n2. **Registro**: Utilizar sistemas eletrônicos apropriados para registrar a transação ou envio de informações. Isso pode ser feito por meio de plataformas proporcionadas pelo BC ou por sistemas próprios das instituições financeiras.\\n\\n3. **Envio**: Transmitir os dados ao BC, respeitando os prazos e formatos exigidos.\\n\\n4. **Confirmação**: Após o envio, é importante verificar a confirmação do recebimento e a conformidade dos dados.\\n\\n5. **Monitoramento**: Manter registros e acompanhar possíveis feedbacks ou solicitações adicionais do BC.\\n\\n### Conclusão\\n\\nO envio de remessas e informações ao Banco Central é um processo fundamental para a transparência e a regulamentação do sistema financeiro brasileiro. Ele garante que as instituições e transações estejam em conformidade com as normas, além de permitir ao BC exercer sua função de supervisão e controle econômico. Para informações mais detalhadas ou específicas, é recomendável consultar diretamente os canais oficiais do Banco Central ou a legislação pertinente.\",\n",
      "        \"refusal\": null\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 22,\n",
      "    \"completion_tokens\": 584,\n",
      "    \"total_tokens\": 606,\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"cached_tokens\": 0,\n",
      "      \"audio_tokens\": 0\n",
      "    },\n",
      "    \"completion_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"system_fingerprint\": \"fp_0705bf87c0\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_KEY = \"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\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "link = \"https://api.openai.com/v1/chat/completions\"\n",
    "id_modelo = \"gpt-4o-mini\"\n",
    "\n",
    "body_message = {\n",
    "    \"model\": id_modelo,\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"me fale um pouco sobre o processo de envio de remessas para o BC\"}]\n",
    "}\n",
    "body_message = json.dumps(body_message)\n",
    "\n",
    "requisicao = requests.post(link, headers=headers, data=body_message)\n",
    "\n",
    "print(requisicao)\n",
    "\n",
    "print(requisicao.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHlLeJ6tmOv"
   },
   "source": [
    "# **ETAPA 2 - Códigos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86MGmtYCONA-"
   },
   "source": [
    "# 2.1 Código para Prompt: Usando o GPT-4-mini para encontrar as perguntas ideais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "collapsed": true,
    "id": "Iq-65aSdLoes",
    "outputId": "0925f1e3-90f3-46aa-ae67-02eb7872bf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo o arquivo: Leiaute de arquivos e base normativa.pdf\n",
      "Lendo o arquivo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Lendo o arquivo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Lendo o arquivo: Sistemas de Negócio.pdf\n",
      "Lendo o arquivo: ARQUIVO_GERAL_UNICAD_0300.pdf\n",
      "Tamanho dos embeddings: 658\n",
      "Bem-vindo! Pergunte sobre os documentos carregados. Digite 'sair' para encerrar.\n",
      "Sua pergunta: Os normativos impactam quais documentos de envio de remessas listados na tabela de Leiaute de arquivos? Exemplo: C204, C209. \n",
      "Resposta: Os normativos mencionados impactam os seguintes documentos de envio de remessas listados na tabela de Leiaute de arquivos:\n",
      "\n",
      "1. **C204** - Envio consolidado – Registro de operações.\n",
      "   - Base normativa: Resolução BCB n° 277, de 31/12/2022 e Instrução Normativa BCB n° 125, de 21/7/2021.\n",
      "\n",
      "2. **C209** - Envio consolidado - Registro de transferências internacionais em reais.\n",
      "   - Base normativa: Resolução BCB n° 277, de 31/12/2022.\n",
      "\n",
      "3. **C220** - eFX - Demais aquisições e transferências.\n",
      "   - Base normativa: Resolução BCB n° 277, de 31/12/2022 e Instrução Normativa BCB n° 159, de 30/9/2021.\n",
      "\n",
      "Esses documentos estão diretamente relacionados às regulamentações estabelecidas nas instruções normativas e resoluções do Banco Central do Brasil, que definem os procedimentos e informações a serem enviadas em remessas.\n",
      "\n",
      "Sua pergunta: Os normativos impactam quais documentos de envio de remessas listados na tabela de Leiaute de arquivos? (Responda conforme o exemplo a seguir) Exemplo: C204, C209. \n",
      "Resposta: Os normativos impactam os seguintes documentos de envio de remessas listados na tabela de Leiaute de arquivos:\n",
      "\n",
      "- C204\n",
      "- C209\n",
      "- C220\n",
      "- 5816\n",
      "- 5817\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5d41d7e5a235>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sua pergunta: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sair\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encerrando. Até mais!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Configuração da API\n",
    "def setup_openai_api():\n",
    "    API_KEY = \"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\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    link = \"https://api.openai.com/v1/chat/completions\"\n",
    "    return headers, link\n",
    "\n",
    "headers, link = setup_openai_api()\n",
    "id_modelo = \"gpt-4o-mini\"\n",
    "\n",
    "# Nome do arquivo PDF específico\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Lista de arquivos a serem processados\n",
    "filenames = [\n",
    "    \"Leiaute de arquivos e base normativa.pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Sistemas de Negócio.pdf\",\n",
    "    \"ARQUIVO_GERAL_UNICAD_0300.pdf\"\n",
    "]\n",
    "\n",
    "# Função para ler múltiplos PDFs\n",
    "def read_multiple_pdfs(file_list):\n",
    "    combined_content = \"\"\n",
    "    document_names = \"\"\n",
    "    for filename in file_list:\n",
    "        try:\n",
    "            print(f\"Lendo o arquivo: {filename}\")\n",
    "            combined_content += read_pdf(filename)  # Reutiliza a função existente `read_pdf`\n",
    "            document_names += f\"{filename}\\n\"  # Adiciona o nome do arquivo ao contexto\n",
    "            combined_content += \"\\n\"  # Adiciona separação entre documentos\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler o arquivo {filename}: {e}\")\n",
    "    return combined_content, document_names\n",
    "\n",
    "file_content, document_names = read_multiple_pdfs(filenames)\n",
    "\n",
    "# Função para carregar e processar os chunks\n",
    "def load_and_chunk_text(text, document_names, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text = f\"Nomes dos documentos:\\n{document_names}\\n\\n\" + text\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evita cortar palavras ao meio\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = load_and_chunk_text(file_content, document_names)\n",
    "\n",
    "# Carregar o modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Gerar embeddings dos chunks\n",
    "chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "\n",
    "# Verifique o tamanho de chunk_embeddings antes de passar para FAISS\n",
    "print(f\"Tamanho dos embeddings: {len(chunk_embeddings)}\")\n",
    "\n",
    "if len(chunk_embeddings) > 0:\n",
    "    # Convertendo embeddings para formato NumPy\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "\n",
    "    # Normalizar os embeddings\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "\n",
    "    # Criar índice FAISS\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "\n",
    "    # Função para fazer a pergunta ao GPT\n",
    "    def ask_gpt(query, context, document_names, model=\"gpt-4o-mini\", max_tokens=450, temperature=0.5, top_p=0.8):\n",
    "        prompt = f\"\"\"\n",
    "        Aqui estão os nomes dos documentos que você está analisando:\n",
    "        {document_names}\n",
    "        Contexto dos documentos:\n",
    "        {context}\n",
    "\n",
    "        Pergunta: {query}\n",
    "        Responda de forma clara e objetiva.\n",
    "        \"\"\"\n",
    "        body_message = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p\n",
    "        }\n",
    "        response = requests.post(link, headers=headers, data=json.dumps(body_message))\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise Exception(f\"Erro na requisição: {response.text}\")\n",
    "\n",
    "    # Exemplo de uso interativo\n",
    "    if __name__ == \"__main__\":\n",
    "        print(\"Bem-vindo! Pergunte sobre os documentos carregados. Digite 'sair' para encerrar.\")\n",
    "\n",
    "        while True:\n",
    "            query = input(\"Sua pergunta: \")\n",
    "            if query.lower() == \"sair\":\n",
    "                print(\"Encerrando. Até mais!\")\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                context = '\\n'.join(chunks)  # Usa todos os chunks para formar o contexto\n",
    "                response = ask_gpt(query, context, document_names)\n",
    "                print(f\"Resposta: {response}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Ocorreu um erro: {e}\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"Erro: Nenhum embedding foi gerado, verifique os PDFs lidos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kysGum4MKRiI"
   },
   "source": [
    "# 2.2 - Código Oficial 1 - Abordagem 1 - GPT-4, RAG E FAISS - 2 CICLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seWWWqHjKROO",
    "outputId": "6c558e47-2088-4ef9-836b-fff8e08c27f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Tabela gerada com sucesso: Tabela_Normativos_Final_RAG.xlsx\n",
      "Erro ao sintetizar a coluna Base Normativa para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Dispõe sobre para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Cargo - Diretor ou empregado responsável para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Instruções de Preenchimento para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Prazo de Envio para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Obrigação do Envio para o documento C204: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Base Normativa para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Revogada para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Dispõe sobre para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Cargo - Diretor ou empregado responsável para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Instruções de Preenchimento para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Prazo de Envio para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Erro ao sintetizar a coluna Obrigação do Envio para o documento C220: 'NoneType' object has no attribute 'search'\n",
      "Tabela sintetizada gerada com sucesso: Tabela_Normativos_Final_RAG_Sintetizada.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pdfplumber\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Configuração da API OpenAI\n",
    "def setup_openai_api():\n",
    "    API_KEY = \"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\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    link = \"https://api.openai.com/v1/chat/completions\"\n",
    "    return headers, link\n",
    "\n",
    "headers, link = setup_openai_api()\n",
    "\n",
    "# Carregar modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do Leiaute\n",
    "def extract_table_from_pdf(leiaute_file):\n",
    "    \"\"\"\n",
    "    Extrai todas as tabelas do Leiaute e retorna um DataFrame consolidado.\n",
    "\n",
    "    Args:\n",
    "        leiaute_file (str): Caminho para o arquivo PDF do Leiaute.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Tabela consolidada contendo todas as informações relevantes.\n",
    "    \"\"\"\n",
    "    tabelas = []\n",
    "    with pdfplumber.open(leiaute_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])  # Converte para DataFrame\n",
    "                    tabelas.append(df)\n",
    "    if tabelas:\n",
    "        return pd.concat(tabelas, ignore_index=True)  # Combina todas as tabelas\n",
    "    else:\n",
    "        return pd.DataFrame()  # Retorna vazio se nenhuma tabela for encontrada\n",
    "\n",
    "# Dividir texto em chunks\n",
    "def load_and_chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evitar cortar palavras\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# Criar índice FAISS\n",
    "def create_faiss_index(chunks):\n",
    "    chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "    return index, chunk_embeddings_np\n",
    "\n",
    "# Obter chunks relevantes\n",
    "def get_relevant_chunks(query, index, chunk_embeddings, chunks, top_k=5):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Fazer perguntas ao GPT usando RAG\n",
    "def ask_gpt_with_rag(query, faiss_index, chunks, top_k=3, model=\"gpt-4\", max_tokens=450, temperature=0.5, top_p=0.8):\n",
    "    # Recuperar chunks relevantes\n",
    "    context_chunks = get_relevant_chunks(query, faiss_index, None, chunks, top_k)\n",
    "    context = \" \".join(context_chunks)\n",
    "\n",
    "    # Formatar o prompt com o contexto recuperado\n",
    "    prompt = f\"\"\"\n",
    "    Baseando-se no seguinte contexto:\n",
    "    {context}\n",
    "\n",
    "    Responda à seguinte pergunta de forma clara, sucinta e objetiva:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "\n",
    "    # Fazer a chamada à API OpenAI\n",
    "    body_message = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p\n",
    "    }\n",
    "    response = requests.post(link, headers=headers, data=json.dumps(body_message))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        raise Exception(f\"Erro na requisição: {response.text}\")\n",
    "\n",
    "# Processar e sintetizar respostas\n",
    "def synthesize_responses(responses):\n",
    "    valid_responses = [resp for resp in responses if \"não fornece\" not in resp.lower() and \"não disponível\" not in resp.lower()]\n",
    "    if valid_responses:\n",
    "        return valid_responses[0]  # Retorna apenas a primeira resposta válida\n",
    "    else:\n",
    "        return \"NÃO\"\n",
    "\n",
    "# Processar normativos e preencher tabela\n",
    "def process_normativos(filenames, leiaute_file, sistemas_negocio_file, responsaveis_file):\n",
    "    # Extrair tabela completa do Leiaute\n",
    "    tabela_leiaute = extract_table_from_pdf(leiaute_file)\n",
    "    if \"Documento\" not in tabela_leiaute.columns:\n",
    "        raise ValueError(\"A coluna 'Documento' não foi encontrada no Leiaute.\")\n",
    "\n",
    "    data = []\n",
    "    responses_by_document = {}\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(f\"Processando normativo: {filename}\")\n",
    "        try:\n",
    "            # Ler e dividir texto em chunks\n",
    "            file_content = read_pdf(filename)\n",
    "            chunks = load_and_chunk_text(file_content)\n",
    "            index, chunk_embeddings = create_faiss_index(chunks)\n",
    "\n",
    "            # Obter os documentos relacionados ao normativo\n",
    "            documentos_relacionados = tabela_leiaute[\n",
    "                tabela_leiaute[\"Base Normativa\"].str.contains(filename.split('.')[0], na=False)\n",
    "            ][\"Documento\"].dropna().unique()\n",
    "\n",
    "            # Perguntas para preencher as colunas\n",
    "            perguntas = {\n",
    "                \"Revogada\": \"Este documento de remessa está atualmente revogado? Responda SIM ou NÃO. Caso não tenha informações suficientes, a resposta é NÃO\",\n",
    "                \"Dispõe sobre\": \"Sobre o que dispõe o Documento de envio de remessas impactado por esse normativo? Exemplos:Relacionando ao documento de remessa C204, pode-se inferir que este documento de envio de remessas deve cumprir todas as regulamentações estabelecidas na resolução do BCB, incluindo o fornecimento de todas as informações necessárias do remetente e o cumprimento dos prazos estipulados para a conversão ou transferência de saldos.\",\n",
    "                \"Cargo - Diretor ou empregado responsável\": \"Qual é o cargo do responsável por este normativo? Responda de forma sucinta. Exemplos: Diretor de Câmbio, Gerente Operacional.\",\n",
    "                \"Instruções de Preenchimento\": \"Onde encontrar as instruções de preenchimento deste normativo? Exemplos: no manual operacional, nas diretrizes da instituição.\",\n",
    "                \"Prazo de Envio\": \"Qual é o prazo de envio relacionado a este normativo? Responda somente em relação aos prazos de envio e suas exceções de forma sucinta Exemplos: até o dia 10 do mês subsequente, dois dias úteis após a solicitação.\",\n",
    "                \"Obrigação do Envio\": \"Para quem é obrigatório o envio relacionado a este normativo? Exemplos: instituições autorizadas a operar câmbio, participantes do sistema PIX.\"\n",
    "             }\n",
    "\n",
    "            # Para cada documento identificado no Leiaute relacionado ao normativo\n",
    "            for documento in documentos_relacionados:\n",
    "                tabela_row = {\n",
    "                    \"Documento\": documento,\n",
    "                    \"Base Normativa\": os.path.splitext(filename)[0]\n",
    "                }\n",
    "\n",
    "                for coluna, pergunta in perguntas.items():\n",
    "                    query = f\"{pergunta} Relacione ao documento de remessa {documento}.\"\n",
    "\n",
    "                    # Use o método RAG para obter a resposta\n",
    "                    resposta = ask_gpt_with_rag(query, index, chunks, top_k=3)\n",
    "\n",
    "                    # Adicione os ifs aqui, dependendo da coluna\n",
    "\n",
    "                    if coluna == \"Prazo de Envio\":\n",
    "                        # Pós-processamento para respostas de prazo de envio\n",
    "                        match = re.search(r\"(até .*?\\.|exceções:.*?\\.|prazos:.*?\\.)\", resposta, re.IGNORECASE)\n",
    "                        resposta = match.group(0).strip() if match else resposta\n",
    "\n",
    "                    elif coluna == \"Cargo - Diretor ou empregado responsável\":\n",
    "                        # Pós-processamento para respostas de cargo\n",
    "                        resposta = re.sub(r\"O cargo.*?é o\", \"\", resposta).strip(\".\")\n",
    "\n",
    "                    tabela_row[coluna] = resposta\n",
    "\n",
    "                data.append(tabela_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "             \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "             \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "             \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "             \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"]\n",
    "\n",
    "# Documentos auxiliares\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "sistemas_negocio_file = \"Sistemas de Negócio.pdf\"\n",
    "responsaveis_file = \"ARQUIVO_GERAL_UNICAD_0300.pdf\"\n",
    "\n",
    "# Executar processamento e salvar tabela\n",
    "tabela_final = process_normativos(filenames, leiaute_file, sistemas_negocio_file, responsaveis_file)\n",
    "tabela_final.to_excel(\"Tabela_Normativos_Final_RAG.xlsx\", index=False)\n",
    "\n",
    "print(\"Tabela gerada com sucesso: Tabela_Normativos_Final_RAG.xlsx\")\n",
    "\n",
    "def synthesize_with_gpt_preserving_columns(tabela):\n",
    "    \"\"\"\n",
    "    Realiza a síntese dos resultados, preservando a estrutura de colunas do DataFrame original.\n",
    "    Cada documento terá valores únicos por coluna.\n",
    "    \"\"\"\n",
    "    synthesized_rows = []\n",
    "\n",
    "    for documento in tabela[\"Documento\"].unique():\n",
    "        # Filtrar as linhas correspondentes ao documento atual\n",
    "        doc_rows = tabela[tabela[\"Documento\"] == documento]\n",
    "\n",
    "        # Criar um dicionário para armazenar as colunas sintetizadas\n",
    "        synthesized_row = {\"Documento\": documento}\n",
    "\n",
    "        # Iterar por cada coluna (exceto 'Documento') e realizar a síntese\n",
    "        for column in tabela.columns:\n",
    "            if column == \"Documento\":\n",
    "                continue  # Não sintetizar a coluna 'Documento'\n",
    "\n",
    "            # Concatenar valores da coluna, ignorando NaNs\n",
    "            column_values = doc_rows[column].dropna().unique()\n",
    "            if len(column_values) > 1:\n",
    "                # Formatar os valores da coluna para enviar ao GPT-4\n",
    "                prompt = f\"Sintetize as seguintes informações da coluna '{column}' para o documento {documento}:\\n\\n\"\n",
    "                prompt += \"\\n\".join(column_values)\n",
    "\n",
    "                try:\n",
    "                    # Fazer interação com GPT-4\n",
    "                    synthesized_value = ask_gpt_with_rag(\n",
    "                        query=prompt,\n",
    "                        faiss_index=None,  # Não é necessário índice FAISS para esta etapa\n",
    "                        chunks=[],\n",
    "                        top_k=3\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao sintetizar a coluna {column} para o documento {documento}: {e}\")\n",
    "                    synthesized_value = \"; \".join(column_values)  # Fallback: juntar valores únicos\n",
    "            else:\n",
    "                synthesized_value = column_values[0] if column_values else \"\"\n",
    "\n",
    "            synthesized_row[column] = synthesized_value\n",
    "\n",
    "        synthesized_rows.append(synthesized_row)\n",
    "\n",
    "    # Retornar um novo DataFrame com os resultados sintetizados\n",
    "    return pd.DataFrame(synthesized_rows)\n",
    "\n",
    "# Adicionar etapa de interação com GPT-4 para síntese final\n",
    "if not tabela_final.empty:\n",
    "    tabela_sintetizada = synthesize_with_gpt_preserving_columns(tabela_final)\n",
    "    tabela_sintetizada.to_excel(\"Tabela_Normativos_Final_RAG_Sintetizada.xlsx\", index=False)\n",
    "    print(\"Tabela sintetizada gerada com sucesso: Tabela_Normativos_Final_RAG_Sintetizada.xlsx\")\n",
    "else:\n",
    "    print(\"Nenhum dado foi gerado para síntese.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFy9cQ4oS-rK"
   },
   "source": [
    "# 2.3.1 Abordagem 2 - Código Oficial 1 - Regras e Heurísticas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45Iomjs_TA16",
    "outputId": "575299bf-3c34-4591-ef95-b8e3a421797e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-afd8f91e61cc>:32: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  match = leiaute_df[leiaute_df['Base Normativa'].str.contains(normative_name, na=False, case=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Tabela gerada com sucesso: Tabela_Normativos_Final_Regras_Enriquecida.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do PDF Leiaute\n",
    "def extract_table_from_pdf(pdf_file):\n",
    "    tabela = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    tabela.extend(table)\n",
    "    # Converter para DataFrame\n",
    "    if tabela:\n",
    "        df = pd.DataFrame(tabela[1:], columns=tabela[0])  # Cabeçalho da primeira linha\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Nenhuma tabela encontrada no arquivo Leiaute.\")\n",
    "\n",
    "# Função para mapear documentos com base no Leiaute\n",
    "def map_documents_to_normatives(leiaute_df, normative_name):\n",
    "    match = leiaute_df[leiaute_df['Base Normativa'].str.contains(normative_name, na=False, case=False)]\n",
    "    if not match.empty:\n",
    "        return \", \".join(match['Documento'].unique())\n",
    "    return \"Não encontrado\"\n",
    "\n",
    "# Função para verificar revogação\n",
    "def check_revogada(text):\n",
    "    if re.search(r\"(revogado|revogada|não está mais em vigor)\", text, re.IGNORECASE):\n",
    "        return \"SIM\"\n",
    "    return \"NÃO\"\n",
    "\n",
    "# Função para dividir texto em chunks\n",
    "def load_and_chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evitar cortar palavras\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# Função para análise baseada em relevância\n",
    "def analyze_relevance(chunks, keywords):\n",
    "    relevance_scores = Counter()\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in chunk.lower():\n",
    "                relevance_scores[i] += 1\n",
    "    if relevance_scores:\n",
    "        # Retornar o chunk com maior relevância\n",
    "        most_relevant = max(relevance_scores, key=relevance_scores.get)\n",
    "        return chunks[most_relevant]\n",
    "    return \"Não encontrado\"\n",
    "\n",
    "# Processar normativos e gerar tabela estruturada\n",
    "def process_normativos_heuristic(filenames, leiaute_file):\n",
    "    # Carregar tabela Leiaute a partir do PDF\n",
    "    leiaute_df = extract_table_from_pdf(leiaute_file)\n",
    "\n",
    "    # Perguntas com padrões regex e palavras-chave\n",
    "    perguntas = {\n",
    "        \"Dispõe sobre\": [\n",
    "            r\"dispõe sobre.*?\\.\",\n",
    "            r\"regulamenta.*?\\.\",\n",
    "            r\"trata de.*?\\.\",\n",
    "            r\"impacta no processo de.*?\\.\",\n",
    "            r\"relaciona-se com.*?\\.\",\n",
    "            r\"estabelece normas sobre.*?\\.\",\n",
    "            r\"determina procedimentos para.*?\\.\",\n",
    "            r\"inclui diretrizes relacionadas a.*?\\.\",\n",
    "            r\"descreve as condições para.*?\\.\",\n",
    "            r\"refere-se a.*?\\.\"\n",
    "        ],\n",
    "        \"Cargo - Diretor ou empregado responsável\": [\n",
    "            r\"diretor[a]? de [^\\.\\n]*\",\n",
    "            r\"gerente [^\\.\\n]*\",\n",
    "            r\"supervisor[a]? de [^\\.\\n]*\",\n",
    "            r\"responsável pela área de [^\\.\\n]*\",\n",
    "            r\"chefe do departamento de [^\\.\\n]*\",\n",
    "            r\"líder da equipe de [^\\.\\n]*\",\n",
    "            r\"coordenador[a]? de [^\\.\\n]*\",\n",
    "            r\"analista responsável por [^\\.\\n]*\",\n",
    "            r\"consultor[a]? de [^\\.\\n]*\",\n",
    "            r\"especialista em [^\\.\\n]*\"\n",
    "        ],\n",
    "        \"Instruções de Preenchimento\": [\n",
    "            r\"instruções de preenchimento.*?\\.\",\n",
    "            r\"manual operacional.*?\\.\",\n",
    "            r\"diretrizes da instituição.*?\\.\",\n",
    "            r\"detalhes para preenchimento.*?\\.\",\n",
    "            r\"informações adicionais sobre.*?\\.\",\n",
    "            r\"orientações disponíveis no.*?\\.\",\n",
    "            r\"documentação de referência.*?\\.\",\n",
    "            r\"guias fornecidos para.*?\\.\",\n",
    "            r\"passo a passo no manual.*?\\.\",\n",
    "            r\"regras de preenchimento encontradas em.*?\\.\"\n",
    "        ],\n",
    "        \"Prazo de Envio\": [\n",
    "            r\"até o dia [0-9]+ do mês subsequente\",\n",
    "            r\"dois dias úteis após a solicitação\",\n",
    "            r\"envio deve ocorrer até.*?\\.\",\n",
    "            r\"prazo limite de envio.*?\\.\",\n",
    "            r\"não pode exceder o dia.*?\\.\",\n",
    "            r\"envio permitido até.*?\\.\",\n",
    "            r\"deve ser enviado até.*?\\.\",\n",
    "            r\"limite máximo para envio.*?\\.\",\n",
    "            r\"obrigatório envio até.*?\\.\",\n",
    "            r\"prazo estipulado é de.*?\\.\"\n",
    "        ],\n",
    "        \"Obrigação do Envio\": [\n",
    "            r\"instituições autorizadas a operar câmbio\",\n",
    "            r\"participantes do sistema PIX\",\n",
    "            r\"entidades regulamentadas pelo BCB\",\n",
    "            r\"obrigação de envio para.*?\\.\",\n",
    "            r\"aplicável a.*?\\.\",\n",
    "            r\"deve ser cumprido por.*?\\.\",\n",
    "            r\"instituições financeiras com.*?\\.\",\n",
    "            r\"responsabilidade recai sobre.*?\\.\",\n",
    "            r\"é obrigatório para.*?\\.\",\n",
    "            r\"norma se aplica a.*?\\.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Processar cada documento no Leiaute\n",
    "    for documento in leiaute_df['Documento'].unique():\n",
    "        normativos = []\n",
    "        tabela_row = {\"Documento\": documento}\n",
    "\n",
    "        for filename in filenames:\n",
    "            print(f\"Processando normativo: {filename}\")\n",
    "            try:\n",
    "                # Ler o texto do arquivo PDF\n",
    "                file_content = read_pdf(filename)\n",
    "                if documento in map_documents_to_normatives(leiaute_df, filename):\n",
    "                    normativos.append(filename)\n",
    "\n",
    "                # Extrair informações com base em perguntas e regex\n",
    "                extracted_info = {coluna: analyze_relevance(load_and_chunk_text(file_content), patterns)\n",
    "                                  for coluna, patterns in perguntas.items()}\n",
    "\n",
    "                # Adicionar informações à linha\n",
    "                tabela_row.update(extracted_info)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "        tabela_row[\"Base Normativa\"] = \", \".join(normativos)\n",
    "        tabela_row[\"Revogada\"] = check_revogada(file_content)\n",
    "\n",
    "        data.append(tabela_row)\n",
    "\n",
    "    # Criar DataFrame com os dados extraídos\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Arquivo Leiaute (PDF)\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "# Processar normativos e gerar tabela\n",
    "tabela_final_heuristic = process_normativos_heuristic(filenames, leiaute_file)\n",
    "\n",
    "# Salvar os resultados em um arquivo Excel\n",
    "output_path = \"Tabela_Normativos_Final_Regras_Enriquecida.xlsx\"\n",
    "tabela_final_heuristic.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Tabela gerada com sucesso: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe2KctgGu7n0"
   },
   "source": [
    "# 2.3.2 Abordagem 2 - Código Oficial 2 - Regras e Heurísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEVOe0KNEyqf",
    "outputId": "6ee588e3-0d36-4cae-a4fa-446eaa184d22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-5d1cbd58f735>:37: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  match = leiaute_df[leiaute_df['Base Normativa'].str.contains(normative_name, na=False, case=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Processando normativo: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Processando normativo: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Processando normativo: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Tabela gerada com sucesso: Tabela_Normativos_Final_Advanced.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-5d1cbd58f735>:154: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tabela_final_advanced = tabela_final_advanced.applymap(clean_text)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Configuração do modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do PDF Leiaute\n",
    "def extract_table_from_pdf(pdf_file):\n",
    "    tabela = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    tabela.extend(table)\n",
    "    if tabela:\n",
    "        df = pd.DataFrame(tabela[1:], columns=tabela[0])  # Cabeçalho da primeira linha\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Nenhuma tabela encontrada no arquivo Leiaute.\")\n",
    "\n",
    "# Função para mapear documentos com base no Leiaute\n",
    "def map_documents_to_normatives(leiaute_df, normative_name):\n",
    "    match = leiaute_df[leiaute_df['Base Normativa'].str.contains(normative_name, na=False, case=False)]\n",
    "    if not match.empty:\n",
    "        return \", \".join(match['Documento'].unique())\n",
    "    return \"Não encontrado\"\n",
    "\n",
    "# Função para verificar revogação\n",
    "def check_revogada(text):\n",
    "    if re.search(r\"(revogado|revogada|não está mais em vigor)\", text, re.IGNORECASE):\n",
    "        return \"SIM\"\n",
    "    return \"NÃO\"\n",
    "\n",
    "# Função para dividir texto em chunks\n",
    "def load_and_chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evitar cortar palavras\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# Função para criar índice FAISS\n",
    "def create_faiss_index(chunks):\n",
    "    chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "    return index, chunks\n",
    "\n",
    "# Função para encontrar chunks relevantes com FAISS\n",
    "def get_relevant_chunks(query, faiss_index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Função para limpar texto e remover caracteres inválidos\n",
    "def clean_text(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('\\n', ' ')  # Substituir quebras de linha por espaço\n",
    "        value = value.replace('\\t', ' ')  # Substituir tabulações por espaço\n",
    "        value = re.sub(r'[^\\x20-\\x7E]', '', value)  # Remover caracteres não imprimíveis\n",
    "        return value.strip()  # Remover espaços extras no início/fim\n",
    "    return value\n",
    "\n",
    "# Processar normativos e gerar tabela estruturada\n",
    "def process_normativos_advanced(filenames, leiaute_file):\n",
    "    leiaute_df = extract_table_from_pdf(leiaute_file)\n",
    "\n",
    "    perguntas = {\n",
    "        \"Dispõe sobre\": \"Sobre o que dispõe o Documento de envio de remessas impactado por esse normativo?\",\n",
    "        \"Cargo - Diretor ou empregado responsável\": \"Qual é o cargo do responsável por este normativo?\",\n",
    "        \"Instruções de Preenchimento\": \"Onde encontrar as instruções de preenchimento deste normativo?\",\n",
    "        \"Prazo de Envio\": \"Qual é o prazo de envio relacionado a este normativo?\",\n",
    "        \"Obrigação do Envio\": \"Para quem é obrigatório o envio relacionado a este normativo?\",\n",
    "    }\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for documento in leiaute_df['Documento'].unique():\n",
    "        normativos = []\n",
    "        tabela_row = {\n",
    "            \"Documento\": documento,\n",
    "            \"Base Normativa\": \"\",\n",
    "            \"Revogada\": \"\",\n",
    "            \"Dispõe sobre\": \"Não encontrado\",\n",
    "            \"Cargo - Diretor ou empregado responsável\": \"Não encontrado\",\n",
    "            \"Instruções de Preenchimento\": \"Não encontrado\",\n",
    "            \"Prazo de Envio\": \"Não encontrado\",\n",
    "            \"Obrigação do Envio\": \"Não encontrado\"\n",
    "        }\n",
    "\n",
    "        for filename in filenames:\n",
    "            print(f\"Processando normativo: {filename}\")\n",
    "            try:\n",
    "                file_content = read_pdf(filename)\n",
    "                chunks = load_and_chunk_text(file_content)\n",
    "                faiss_index, processed_chunks = create_faiss_index(chunks)\n",
    "\n",
    "                if documento in map_documents_to_normatives(leiaute_df, filename):\n",
    "                    normativos.append(filename)\n",
    "\n",
    "                    for coluna, pergunta in perguntas.items():\n",
    "                        relevant_chunks = get_relevant_chunks(pergunta, faiss_index, processed_chunks, top_k=3)\n",
    "                        resposta = \" \".join(relevant_chunks).strip()\n",
    "                        if resposta:\n",
    "                            tabela_row[coluna] = resposta\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "        tabela_row[\"Base Normativa\"] = \", \".join(normativos)\n",
    "        tabela_row[\"Revogada\"] = check_revogada(file_content)\n",
    "        data.append(tabela_row)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Arquivo Leiaute (PDF)\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "# Processar normativos e gerar tabela\n",
    "tabela_final_advanced = process_normativos_advanced(filenames, leiaute_file)\n",
    "\n",
    "# Limpar texto em todas as células do DataFrame\n",
    "tabela_final_advanced = tabela_final_advanced.applymap(clean_text)\n",
    "\n",
    "# Salvar os resultados em um arquivo Excel\n",
    "output_path = \"Tabela_Normativos_Final_Advanced.xlsx\"\n",
    "tabela_final_advanced.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Tabela gerada com sucesso: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_HWpev_pr-Z"
   },
   "source": [
    "# 2.3.3.1 Abordagem 2 - Código Oficial 3 - Regras e Heurísticas + FAISS e RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGMZICEYpx4p",
    "outputId": "2e52a967-7375-429b-d9df-f8262a075210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Tabela gerada com sucesso: Tabela_Normativos_Final_RAG_Enriquecida.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Configuração do modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do PDF Leiaute\n",
    "def extract_table_from_pdf(pdf_file):\n",
    "    tabela = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    tabela.extend(table)\n",
    "    # Converter para DataFrame\n",
    "    if tabela:\n",
    "        df = pd.DataFrame(tabela[1:], columns=tabela[0])  # Cabeçalho da primeira linha\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Nenhuma tabela encontrada no arquivo Leiaute.\")\n",
    "\n",
    "# Função para criar índices FAISS com embeddings\n",
    "def create_faiss_index(chunks):\n",
    "    chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "    return index, chunks\n",
    "\n",
    "# Função para encontrar chunks relevantes com FAISS\n",
    "def get_relevant_chunks(query, faiss_index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Processar normativos com busca contextual\n",
    "def process_normativos_rag(filenames, leiaute_file):\n",
    "    # Carregar tabela Leiaute a partir do PDF\n",
    "    leiaute_df = extract_table_from_pdf(leiaute_file)\n",
    "    perguntas = {\n",
    "        \"Dispõe sobre\": \"Sobre o que dispõe o Documento de envio de remessas impactado por esse normativo?\",\n",
    "        \"Cargo - Diretor ou empregado responsável\": \"Qual é o cargo do responsável por este normativo?\",\n",
    "        \"Instruções de Preenchimento\": \"Onde encontrar as instruções de preenchimento deste normativo?\",\n",
    "        \"Prazo de Envio\": \"Qual é o prazo de envio relacionado a este normativo?\",\n",
    "        \"Obrigação do Envio\": \"Para quem é obrigatório o envio relacionado a este normativo?\",\n",
    "    }\n",
    "    data = []\n",
    "\n",
    "    # Processar cada documento no Leiaute\n",
    "    for documento in leiaute_df['Documento'].unique():\n",
    "        normativos = []\n",
    "        tabela_row = {\"Documento\": documento}\n",
    "\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # Ler texto do arquivo PDF e dividir em chunks\n",
    "                file_content = read_pdf(filename)\n",
    "                chunks = [chunk.strip() for chunk in file_content.split(\"\\n\") if chunk.strip()]\n",
    "                faiss_index, processed_chunks = create_faiss_index(chunks)\n",
    "\n",
    "                if documento in map_documents_to_normatives(leiaute_df, filename):\n",
    "                    normativos.append(filename)\n",
    "\n",
    "                    for coluna, pergunta in perguntas.items():\n",
    "                        # Buscar chunks relevantes e formar a resposta\n",
    "                        relevant_chunks = get_relevant_chunks(pergunta, faiss_index, processed_chunks, top_k=3)\n",
    "                        resposta = \" \".join(relevant_chunks).strip()\n",
    "                        tabela_row[coluna] = resposta if resposta else \"Não encontrado\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "        tabela_row[\"Base Normativa\"] = \", \".join(normativos)\n",
    "        tabela_row[\"Revogada\"] = check_revogada(file_content)\n",
    "        data.append(tabela_row)\n",
    "\n",
    "    # Criar DataFrame com os dados extraídos\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Arquivo Leiaute (PDF)\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "# Processar normativos e gerar tabela\n",
    "tabela_final_rag = process_normativos_rag(filenames, leiaute_file)\n",
    "\n",
    "# Salvar os resultados em um arquivo Excel\n",
    "output_path = \"Tabela_Normativos_Final_RAG_Enriquecida.xlsx\"\n",
    "tabela_final_rag.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Tabela gerada com sucesso: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbfzOveovcIW"
   },
   "source": [
    "# 2.3.3.2 Abordagem 2 - Código Oficial 3 - Regras e Heurísticas + FAISS e RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcHGQvQGwvk7",
    "outputId": "09ff0157-d806-47db-fdc8-f94ee6f1e638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a1e192dbd19e>:139: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tabela_final_rag = tabela_final_rag.applymap(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela gerada com sucesso: Tabela_Normativos_Final_RAG_Enriquecida.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Configuração do modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do PDF Leiaute\n",
    "def extract_table_from_pdf(pdf_file):\n",
    "    tabela = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    tabela.extend(table)\n",
    "    # Converter para DataFrame\n",
    "    if tabela:\n",
    "        df = pd.DataFrame(tabela[1:], columns=tabela[0])  # Cabeçalho da primeira linha\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Nenhuma tabela encontrada no arquivo Leiaute.\")\n",
    "\n",
    "# Função para criar índices FAISS com embeddings\n",
    "def create_faiss_index(chunks):\n",
    "    chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "    return index, chunks\n",
    "\n",
    "# Função para encontrar chunks relevantes com FAISS\n",
    "def get_relevant_chunks(query, faiss_index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Função para verificar revogação\n",
    "def check_revogada(text):\n",
    "    if re.search(r\"(revogado|revogada|não está mais em vigor)\", text, re.IGNORECASE):\n",
    "        return \"SIM\"\n",
    "    return \"NÃO\"\n",
    "\n",
    "# Função para dividir texto em chunks\n",
    "def load_and_chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evitar cortar palavras\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# Função para limpar texto\n",
    "def clean_text(value):\n",
    "    if isinstance(value, str):\n",
    "        # Substituir caracteres ilegais e quebras de linha\n",
    "        value = value.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "        # Remover caracteres não imprimíveis\n",
    "        value = re.sub(r'[^\\x20-\\x7E]', '', value)\n",
    "    return value\n",
    "\n",
    "# Processar normativos com busca contextual\n",
    "def process_normativos_rag(filenames, leiaute_file):\n",
    "    # Carregar tabela Leiaute a partir do PDF\n",
    "    leiaute_df = extract_table_from_pdf(leiaute_file)\n",
    "    perguntas = {\n",
    "        \"Dispõe sobre\": \"Sobre o que dispõe o Documento de envio de remessas impactado por esse normativo?\",\n",
    "        \"Cargo - Diretor ou empregado responsável\": \"Qual é o cargo do responsável por este normativo?\",\n",
    "        \"Instruções de Preenchimento\": \"Onde encontrar as instruções de preenchimento deste normativo?\",\n",
    "        \"Prazo de Envio\": \"Qual é o prazo de envio relacionado a este normativo?\",\n",
    "        \"Obrigação do Envio\": \"Para quem é obrigatório o envio relacionado a este normativo?\",\n",
    "    }\n",
    "    data = []\n",
    "\n",
    "    # Processar cada documento no Leiaute\n",
    "    for documento in leiaute_df['Documento'].unique():\n",
    "        normativos = []\n",
    "        tabela_row = {\"Documento\": documento}\n",
    "\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # Ler texto do arquivo PDF e dividir em chunks\n",
    "                file_content = read_pdf(filename)\n",
    "                chunks = [chunk.strip() for chunk in file_content.split(\"\\n\") if chunk.strip()]\n",
    "                faiss_index, processed_chunks = create_faiss_index(chunks)\n",
    "\n",
    "                if documento in map_documents_to_normatives(leiaute_df, filename):\n",
    "                    normativos.append(filename)\n",
    "\n",
    "                    for coluna, pergunta in perguntas.items():\n",
    "                        # Buscar chunks relevantes e formar a resposta\n",
    "                        relevant_chunks = get_relevant_chunks(pergunta, faiss_index, processed_chunks, top_k=3)\n",
    "                        resposta = \" \".join(relevant_chunks).strip()\n",
    "                        tabela_row[coluna] = resposta if resposta else \"Não encontrado\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "        tabela_row[\"Base Normativa\"] = \", \".join(normativos)\n",
    "        tabela_row[\"Revogada\"] = check_revogada(file_content)\n",
    "        data.append(tabela_row)\n",
    "\n",
    "    # Criar DataFrame com os dados extraídos\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Arquivo Leiaute (PDF)\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "# Processar normativos e gerar tabela\n",
    "tabela_final_rag = process_normativos_rag(filenames, leiaute_file)\n",
    "\n",
    "# Antes de salvar, limpar todos os dados do DataFrame\n",
    "tabela_final_rag = tabela_final_rag.applymap(clean_text)\n",
    "\n",
    "# Salvar os resultados em um arquivo Excel\n",
    "output_path = \"Tabela_Normativos_Final_RAG_Enriquecida.xlsx\"\n",
    "tabela_final_rag.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Tabela gerada com sucesso: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJODXv7Zv9Wx"
   },
   "source": [
    "# 2.3.3.1 Abordagem 2 - Código Oficial 3 - Regras e Heurísticas + FAISS e RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAGj5C5qQudy",
    "outputId": "353e0205-0d0b-40d4-940e-19e0e8635730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisando palavras-chave em: Comunicado n° 32.864 de 7.12.2018.pdf\n",
      "Analisando palavras-chave em: Resolução BCB n° 277 de 31.12.2022.pdf\n",
      "Analisando palavras-chave em: Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\n",
      "Analisando palavras-chave em: Instrução Normativa BCB n° 159 de 30.9.2021.pdf\n",
      "Analisando palavras-chave em: Instrução Normativa BCB n° 296 de 22.8.2022.pdf\n",
      "Palavras-chave frequentes por coluna: {'Dispõe sobre': ['câmbio', 'para', 'operação', 'resolução', 'operações', 'moeda', 'estrangeira', 'banco', 'brasil', 'central'], 'Cargo - Diretor ou empregado responsável': ['câmbio', 'para', 'operação', 'resolução', 'operações', 'moeda', 'estrangeira', 'banco', 'brasil', 'central'], 'Instruções de Preenchimento': ['câmbio', 'para', 'operação', 'resolução', 'operações', 'moeda', 'estrangeira', 'banco', 'brasil', 'central'], 'Prazo de Envio': ['câmbio', 'para', 'operação', 'resolução', 'operações', 'moeda', 'estrangeira', 'banco', 'brasil', 'central'], 'Obrigação do Envio': ['câmbio', 'para', 'operação', 'resolução', 'operações', 'moeda', 'estrangeira', 'banco', 'brasil', 'central']}\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Comunicado n° 32.864 de 7.12.2018.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Resolução BCB n° 277 de 31.12.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 159 de 30.9.2021.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Erro ao processar o normativo Instrução Normativa BCB n° 296 de 22.8.2022.pdf: name 'map_documents_to_normatives' is not defined\n",
      "Tabela gerada com sucesso: Tabela_Normativos_Final_Adaptativa.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8a6c5215387e>:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  tabela_final_adaptive = tabela_final_adaptive.applymap(clean_text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from collections import Counter\n",
    "\n",
    "# Configuração do modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Função para ler PDFs\n",
    "def read_pdf(file_path):\n",
    "    file_content = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            file_content += page.extract_text()\n",
    "    return file_content\n",
    "\n",
    "# Função para extrair tabela do PDF Leiaute\n",
    "def extract_table_from_pdf(pdf_file):\n",
    "    tabela = []\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table:\n",
    "                    tabela.extend(table)\n",
    "    # Converter para DataFrame\n",
    "    if tabela:\n",
    "        df = pd.DataFrame(tabela[1:], columns=tabela[0])  # Cabeçalho da primeira linha\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(\"Nenhuma tabela encontrada no arquivo Leiaute.\")\n",
    "\n",
    "# Função para criar índices FAISS com embeddings\n",
    "def create_faiss_index(chunks):\n",
    "    chunk_embeddings = embedding_model.encode(chunks, convert_to_tensor=False)\n",
    "    chunk_embeddings_np = np.array(chunk_embeddings)\n",
    "    faiss.normalize_L2(chunk_embeddings_np)\n",
    "    index = faiss.IndexFlatL2(chunk_embeddings_np.shape[1])\n",
    "    index.add(chunk_embeddings_np)\n",
    "    return index, chunks\n",
    "\n",
    "# Função para encontrar chunks relevantes com FAISS\n",
    "def get_relevant_chunks(query, faiss_index, chunks, top_k=3):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Função para verificar revogação\n",
    "def check_revogada(text):\n",
    "    if re.search(r\"(revogado|revogada|não está mais em vigor)\", text, re.IGNORECASE):\n",
    "        return \"SIM\"\n",
    "    return \"NÃO\"\n",
    "\n",
    "# Função para dividir texto em chunks\n",
    "def load_and_chunk_text(text, chunk_size=300, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        if end < len(text):\n",
    "            end = text.rfind(' ', start, end) + 1  # Evitar cortar palavras\n",
    "        chunks.append(text[start:end])\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# Função para identificar palavras-chave frequentes\n",
    "def identify_frequent_terms(filenames, perguntas):\n",
    "    term_frequencies = {coluna: Counter() for coluna in perguntas.keys()}\n",
    "\n",
    "    for filename in filenames:\n",
    "        print(f\"Analisando palavras-chave em: {filename}\")\n",
    "        try:\n",
    "            file_content = read_pdf(filename)\n",
    "            for coluna, pergunta in perguntas.items():\n",
    "                # Dividir texto em palavras e filtrar stopwords simples\n",
    "                words = re.findall(r'\\b\\w+\\b', file_content.lower())\n",
    "                for word in words:\n",
    "                    if len(word) > 3:  # Ignorar palavras curtas\n",
    "                        term_frequencies[coluna][word] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao analisar {filename}: {e}\")\n",
    "\n",
    "    # Selecionar os termos mais frequentes para cada coluna\n",
    "    frequent_terms = {\n",
    "        coluna: [word for word, _ in term_frequencies[coluna].most_common(10)]\n",
    "        for coluna in perguntas.keys()\n",
    "    }\n",
    "    return frequent_terms\n",
    "\n",
    "def process_normativos_adaptive(filenames, leiaute_file):\n",
    "    # Carregar tabela Leiaute a partir do PDF\n",
    "    leiaute_df = extract_table_from_pdf(leiaute_file)\n",
    "    perguntas = {\n",
    "        \"Dispõe sobre\": \"Sobre o que dispõe o Documento de envio de remessas impactado por esse normativo?\",\n",
    "        \"Cargo - Diretor ou empregado responsável\": \"Qual é o cargo do responsável por este normativo?\",\n",
    "        \"Instruções de Preenchimento\": \"Onde encontrar as instruções de preenchimento deste normativo?\",\n",
    "        \"Prazo de Envio\": \"Qual é o prazo de envio relacionado a este normativo?\",\n",
    "        \"Obrigação do Envio\": \"Para quem é obrigatório o envio relacionado a este normativo?\",\n",
    "    }\n",
    "\n",
    "    # Identificar palavras-chave frequentes\n",
    "    frequent_terms = identify_frequent_terms(filenames, perguntas)\n",
    "    print(\"Palavras-chave frequentes por coluna:\", frequent_terms)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for documento in leiaute_df['Documento'].unique():\n",
    "        normativos = []\n",
    "        tabela_row = {\n",
    "            \"Documento\": documento,\n",
    "            \"Base Normativa\": \"\",\n",
    "            \"Revogada\": \"\",\n",
    "            \"Dispõe sobre\": \"Não encontrado\",\n",
    "            \"Cargo - Diretor ou empregado responsável\": \"Não encontrado\",\n",
    "            \"Instruções de Preenchimento\": \"Não encontrado\",\n",
    "            \"Prazo de Envio\": \"Não encontrado\",\n",
    "            \"Obrigação do Envio\": \"Não encontrado\"\n",
    "        }\n",
    "\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # Ler texto do arquivo PDF e dividir em chunks\n",
    "                file_content = read_pdf(filename)\n",
    "                chunks = [chunk.strip() for chunk in file_content.split(\"\\n\") if chunk.strip()]\n",
    "                faiss_index, processed_chunks = create_faiss_index(chunks)\n",
    "\n",
    "                # Adicionar normativos relacionados ao documento\n",
    "                if documento in map_documents_to_normatives(leiaute_df, filename):\n",
    "                    normativos.append(filename)\n",
    "\n",
    "                    # Processar cada pergunta\n",
    "                    for coluna, pergunta in perguntas.items():\n",
    "                        # Incorporar palavras-chave frequentes à pergunta\n",
    "                        query = f\"{pergunta} Palavras-chave: {', '.join(frequent_terms[coluna])}.\"\n",
    "                        relevant_chunks = get_relevant_chunks(query, faiss_index, processed_chunks, top_k=3)\n",
    "                        resposta = \" \".join(relevant_chunks).strip()\n",
    "                        if resposta:\n",
    "                            tabela_row[coluna] = resposta\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o normativo {filename}: {e}\")\n",
    "\n",
    "        # Atualizar valores no dicionário\n",
    "        tabela_row[\"Base Normativa\"] = \", \".join(normativos)\n",
    "        tabela_row[\"Revogada\"] = check_revogada(file_content)\n",
    "\n",
    "        data.append(tabela_row)\n",
    "\n",
    "    # Criar DataFrame com os dados extraídos\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Lista de arquivos PDF normativos\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Arquivo Leiaute (PDF)\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "# Processar normativos e gerar tabela\n",
    "tabela_final_adaptive = process_normativos_adaptive(filenames, leiaute_file)\n",
    "\n",
    "# Antes de salvar, limpar todos os dados do DataFrame\n",
    "tabela_final_adaptive = tabela_final_adaptive.applymap(clean_text)\n",
    "\n",
    "# Salvar os resultados em um arquivo Excel\n",
    "output_path = \"Tabela_Normativos_Final_Adaptativa.xlsx\"\n",
    "tabela_final_adaptive.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"Tabela gerada com sucesso: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr3tMt8S0wOA"
   },
   "source": [
    "# 2.4 Abordagem 3 - Código Oficial 1 - Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzT4yoSf4l-C"
   },
   "source": [
    "\n",
    "*   Script 1: Limpeza, tokenização, divisão dos dados\n",
    "*   Script 2: Carregamento e configuração do modelo BERT\n",
    "*   Script 3: Treinamento e fine-tuning do modelo\n",
    "*   Script 4: Avaliação do modelo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9npQsNq9ATk6"
   },
   "source": [
    "# 2.4.1 Preparação dos dados para o BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lRsiP925UnC",
    "outputId": "dfc0b268-d17a-4955-c219-ad8cdd4be0c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados processados salvos com sucesso em 'processed_data_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Inicializar o tokenizer do BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "pdf_directory = './'  # Diretório onde estão armazenados os PDFs\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "\n",
    "def read_and_tokenize_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        full_text = ''\n",
    "        for page in pdf.pages:\n",
    "            full_text += page.extract_text() or ''\n",
    "    # Limpeza básica e tokenização\n",
    "    full_text = ' '.join(full_text.split())\n",
    "    return tokenizer(full_text, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Processar documentos normativos e auxiliares\n",
    "data = {'filename': [], 'label': [], 'input_ids': [], 'attention_mask': []}\n",
    "for filename in filenames:\n",
    "    encoding = read_and_tokenize_pdf(os.path.join(pdf_directory, filename))\n",
    "    data['filename'].append(filename)\n",
    "    data['input_ids'].append(encoding['input_ids'][0])\n",
    "    data['attention_mask'].append(encoding['attention_mask'][0])\n",
    "    label = 1 if 'REVOGADO' in filename else 0\n",
    "    data['label'].append(label)\n",
    "\n",
    "# Converter listas para tensores do PyTorch\n",
    "data['input_ids'] = torch.stack(data['input_ids'])\n",
    "data['attention_mask'] = torch.stack(data['attention_mask'])\n",
    "data['labels'] = torch.tensor(data['label'])\n",
    "\n",
    "# Criar DataFrame para exportação\n",
    "df = pd.DataFrame({\n",
    "    'Filename': data['filename'],\n",
    "    'Label': data['label']\n",
    "})\n",
    "\n",
    "# Salvar o DataFrame em um arquivo CSV\n",
    "df.to_csv('processed_data_results.csv', index=False)\n",
    "\n",
    "print(\"Resultados processados salvos com sucesso em 'processed_data_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4oOwNZd02mo",
    "outputId": "67499392-5b0d-4a1e-8589-0a357f872851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiras linhas das tabelas extraídas do documento de leiaute:\n",
      "                                      Base Normativa  \\\n",
      "0             Resolução BCB n° 277 de 31.12.2022.pdf   \n",
      "1  Instrução Normativa BCB n° 125 de 21.7.2021 (R...   \n",
      "2             Resolução BCB n° 277 de 31.12.2022.pdf   \n",
      "3             Resolução BCB n° 277 de 31.12.2022.pdf   \n",
      "4    Instrução Normativa BCB n° 159 de 30.9.2021.pdf   \n",
      "\n",
      "                                    Conteúdo/Assunto Documento  \n",
      "0  Envio consolidado – Registro de\\noperações – A...      C204  \n",
      "1  Envio consolidado – Registro de\\noperações – A...      C204  \n",
      "2  Envio consolidado - Registro de\\ntransferência...      C209  \n",
      "3          eFX - Demais aquisições e\\ntransferências      C220  \n",
      "4          eFX - Demais aquisições e\\ntransferências      C220  \n",
      "Detalhes dos documentos processados salvos com sucesso em 'document_classification_details.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Inicializar o tokenizer do BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "pdf_directory = './'  # Diretório onde estão armazenados os PDFs\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "# Caminhos para os documentos auxiliares\n",
    "leiaute_file = \"Leiaute de arquivos e base normativa 5.pdf\"\n",
    "sistemas_negocio_file = \"Sistemas de Negócio.pdf\"\n",
    "responsaveis_file = \"ARQUIVO_GERAL_UNICAD_0300.pdf\"\n",
    "\n",
    "def read_and_tokenize_pdf(file_path, return_text=False):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        full_text = ''\n",
    "        for page in pdf.pages:\n",
    "            full_text += page.extract_text() or ''\n",
    "    cleaned_text = ' '.join(full_text.split())\n",
    "    if return_text:\n",
    "        return cleaned_text  # Retorna o texto limpo para ser usado como informação\n",
    "    return tokenizer(cleaned_text, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Função para extrair tabelas de PDF\n",
    "def extract_tables_from_pdf(file_path):\n",
    "    tabelas = []\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for table in page.extract_tables():\n",
    "                df = pd.DataFrame(table[1:], columns=table[0])  # Primeira linha como cabeçalho\n",
    "                tabelas.append(df)\n",
    "    return pd.concat(tabelas, ignore_index=True) if tabelas else pd.DataFrame()\n",
    "\n",
    "# Processar documentos normativos e auxiliares\n",
    "data = {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "for filename in filenames:\n",
    "    encoding = read_and_tokenize_pdf(os.path.join(pdf_directory, filename))\n",
    "    data['input_ids'].append(encoding['input_ids'][0])\n",
    "    data['attention_mask'].append(encoding['attention_mask'][0])\n",
    "    label = 1 if 'REVOGADO' in filename else 0\n",
    "    data['labels'].append(label)\n",
    "\n",
    "# Processar documentos auxiliares\n",
    "leiaute_table = extract_tables_from_pdf(os.path.join(pdf_directory, leiaute_file))\n",
    "# Aqui, leiaute_table contém as tabelas extraídas que podem ser utilizadas conforme a necessidade\n",
    "\n",
    "# Converter listas para tensores do PyTorch\n",
    "data['input_ids'] = torch.stack(data['input_ids'])\n",
    "data['attention_mask'] = torch.stack(data['attention_mask'])\n",
    "data['labels'] = torch.tensor(data['labels'])\n",
    "\n",
    "# Criação do Dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Exemplo de utilização de dados extraídos dos documentos auxiliares\n",
    "print(\"Primeiras linhas das tabelas extraídas do documento de leiaute:\")\n",
    "print(leiaute_table.head())\n",
    "\n",
    "# Estruturar dados incluindo textos extraídos\n",
    "results_data = {\n",
    "    'Filename': filenames,\n",
    "    'Label': data['labels'],\n",
    "    'Extracted_Text': []  # Armazenar textos extraídos\n",
    "}\n",
    "\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(pdf_directory, filename)\n",
    "    extracted_text = read_and_tokenize_pdf(file_path, return_text=True)  # Ajustar função para retornar texto\n",
    "    results_data['Extracted_Text'].append(extracted_text)\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Salvar os resultados em um arquivo CSV\n",
    "results_df.to_csv('document_classification_details.csv', index=False)\n",
    "\n",
    "print(\"Detalhes dos documentos processados salvos com sucesso em 'document_classification_details.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924,
     "referenced_widgets": [
      "f53c10655cc44e45a0b9438f45bfb516",
      "be05f63792d74cb68be4026f95efe9be",
      "83f1188914de41d1aca3e1c6ffde2a37",
      "3d8fdaf674cf479997ad7d0461e53d8c",
      "b4db37870e97495eb9e993c7ab1d2196",
      "f5cad469b96447ef8f57b80828904d88",
      "8be39fc10166469987f0c19a114e473d",
      "6d95c58dcf404763b4f7819524a9b693",
      "c51e21c87b114a0f86d439f7cfd4cc55",
      "e79d15f42f044caea9bdcd112e42d909",
      "e57996801f2244a3a6f2cff4ddabaad3"
     ]
    },
    "id": "ft0ACC_mIUvz",
    "outputId": "da834b58-79d0-4f7e-8712-932b12d3e57b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53c10655cc44e45a0b9438f45bfb516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n",
      "\n"
     ]
    },
    {
     "ename": "Abort",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bae6b8b5bd15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Treinar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Salvar o modelo treinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2430\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    838\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should never get here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             )\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbort\u001b[0m: "
     ]
    },
    {
     "ename": "Abort",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bae6b8b5bd15>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Treinar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Salvar o modelo treinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2430\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    520\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    838\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;31m# Need to build delay into this sentry capture because our exit hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;31m# mess with sentry's ability to send out errors before the program ends.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# should never get here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# this will messily add this \"reraise\" function to the stack trace,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# but hopefully it's not too bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_safe_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1282\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;34mf\"You can find your API key in your browser here: {app_url}/authorize\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             )\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAbort\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializar o tokenizer do BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "pdf_directory = './'  # Diretório onde estão armazenados os PDFs\n",
    "filenames = [\n",
    "    \"Comunicado n° 32.864 de 7.12.2018.pdf\",\n",
    "    \"Resolução BCB n° 277 de 31.12.2022.pdf\",\n",
    "    \"Instrução Normativa BCB n° 125 de 21.7.2021 (REVOGADO).pdf\",\n",
    "    \"Instrução Normativa BCB n° 159 de 30.9.2021.pdf\",\n",
    "    \"Instrução Normativa BCB n° 296 de 22.8.2022.pdf\"\n",
    "]\n",
    "\n",
    "def read_and_tokenize_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        full_text = ''\n",
    "        for page in pdf.pages:\n",
    "            full_text += page.extract_text() or ''\n",
    "    full_text = ' '.join(full_text.split())\n",
    "    return tokenizer(full_text, truncation=True, padding='max_length', max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Processar documentos normativos e auxiliares\n",
    "inputs = []\n",
    "labels = []  # Define labels here based on your classification needs\n",
    "for filename in filenames:\n",
    "    file_path = os.path.join(pdf_directory, filename)\n",
    "    tokenized_input = read_and_tokenize_pdf(file_path)\n",
    "    inputs.append(tokenized_input)\n",
    "    labels.append(1 if 'REVOGADO' in filename else 0)\n",
    "\n",
    "# Preparação do Dataset\n",
    "input_ids = torch.cat([input['input_ids'] for input in inputs], dim=0)\n",
    "attention_masks = torch.cat([input['attention_mask'] for input in inputs], dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_masks,\n",
    "    'labels': labels\n",
    "})\n",
    "\n",
    "# Configuração e treinamento do modelo BERT (Exemplo de Configuração)\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # onde salvar os resultados do treinamento\n",
    "    num_train_epochs=3,              # número total de épocas de treinamento\n",
    "    per_device_train_batch_size=8,   # tamanho do batch por dispositivo\n",
    "    warmup_steps=500,                # número de passos de aquecimento antes de começar a diminuir a taxa de aprendizado\n",
    "    weight_decay=0.01,               # decaimento do peso para ajudar a evitar overfitting\n",
    "    logging_dir='./logs',            # diretório para armazenar os logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,  # Dataset para treinamento\n",
    "    eval_dataset=dataset    # Dataset para avaliação\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "trainer.save_model('./bert_trained_model')\n",
    "\n",
    "# Exportar resultados para verificação\n",
    "df = pd.DataFrame({\n",
    "    'Filename': filenames,\n",
    "    'Label': labels.tolist()\n",
    "})\n",
    "df.to_csv('processed_results.csv', index=False)\n",
    "\n",
    "print(\"Modelo treinado e resultados processados salvos com sucesso.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3d8fdaf674cf479997ad7d0461e53d8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e79d15f42f044caea9bdcd112e42d909",
      "placeholder": "​",
      "style": "IPY_MODEL_e57996801f2244a3a6f2cff4ddabaad3",
      "value": " 440M/440M [00:03&lt;00:00, 115MB/s]"
     }
    },
    "6d95c58dcf404763b4f7819524a9b693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83f1188914de41d1aca3e1c6ffde2a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d95c58dcf404763b4f7819524a9b693",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c51e21c87b114a0f86d439f7cfd4cc55",
      "value": 440449768
     }
    },
    "8be39fc10166469987f0c19a114e473d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4db37870e97495eb9e993c7ab1d2196": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be05f63792d74cb68be4026f95efe9be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5cad469b96447ef8f57b80828904d88",
      "placeholder": "​",
      "style": "IPY_MODEL_8be39fc10166469987f0c19a114e473d",
      "value": "model.safetensors: 100%"
     }
    },
    "c51e21c87b114a0f86d439f7cfd4cc55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e57996801f2244a3a6f2cff4ddabaad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e79d15f42f044caea9bdcd112e42d909": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f53c10655cc44e45a0b9438f45bfb516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be05f63792d74cb68be4026f95efe9be",
       "IPY_MODEL_83f1188914de41d1aca3e1c6ffde2a37",
       "IPY_MODEL_3d8fdaf674cf479997ad7d0461e53d8c"
      ],
      "layout": "IPY_MODEL_b4db37870e97495eb9e993c7ab1d2196"
     }
    },
    "f5cad469b96447ef8f57b80828904d88": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
